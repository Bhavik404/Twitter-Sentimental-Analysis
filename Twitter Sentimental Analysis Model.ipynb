{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Wadiya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#for data wrangling and manipulation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#for NLP text processing and formatting\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# For word lemmitization\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# for word Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# for Machine Learning process\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# for Machine Learning model evaluation\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Global Parameters\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen mani movi movi good\n"
     ]
    }
   ],
   "source": [
    "# def preprocess_tweet_text(tweet):\n",
    "#     \"\"\"\n",
    "#     Function to process the the tweet text and tranform it into format usable by Machine learning models\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # to convert all the characters of the tweet into lower case alphabets\n",
    "#     tweet.lower()\n",
    "    \n",
    "#     # Remove urls from the tweets\n",
    "#     tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
    "    \n",
    "#     # Remove user related references from the tweets:: '@' and '#' \n",
    "#     tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
    "    \n",
    "#     # Remove punctuations from the tweets\n",
    "#     tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "#     # Remove stopwords from the tweets\n",
    "#     tweet_tokens = word_tokenize(tweet)\n",
    "#     filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
    "#     joined_text = \" \".join(filtered_words)\n",
    "    \n",
    "#     return joined_text\n",
    "def preprocess_tweet_text(tweet):\n",
    "    #convert all text to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    #remove urls\n",
    "    tweet = re.sub(r\"http\\s+|www\\s+|https\\S+\",\"\",tweet,flags=re.MULTILINE)\n",
    "    #remove puntuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    #remove user @ references and \"#\" from tweet\n",
    "    tweet = re.sub(r\"\\@\\w+|\\#\",\"\",tweet)\n",
    "    #remove stopwords\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [word for word in tweet_tokens if word not in stop_words]\n",
    "    #stemming\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "    #lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w,pos = \"a\") for w in stemmed_words]\n",
    "\n",
    "    return \" \".join(lemma_words)\n",
    "\n",
    "print(preprocess_tweet_text(\"I have seen many movies but this movie was not good\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(train_fit):\n",
    "    \"\"\"\n",
    "    Function to Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "    TF-IDF - Term Frequency Inverse Documnet Frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    vector = TfidfVectorizer(sublinear_tf=True)# Defining the vector\n",
    "    vector.fit(train_fit)# fitting the data into the vector\n",
    "    return vector # returning the vector as function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      textID  text  sentiment  Time of Tweet  Age of User  Country  \\\n",
      "0       True  True       True           True         True     True   \n",
      "1       True  True       True           True         True     True   \n",
      "2       True  True       True           True         True     True   \n",
      "3       True  True       True           True         True     True   \n",
      "4       True  True       True           True         True     True   \n",
      "...      ...   ...        ...            ...          ...      ...   \n",
      "3529    True  True       True           True         True     True   \n",
      "3530    True  True       True           True         True     True   \n",
      "3531    True  True       True           True         True     True   \n",
      "3532    True  True       True           True         True     True   \n",
      "3533    True  True       True           True         True     True   \n",
      "\n",
      "      Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
      "0                 True             True             True  \n",
      "1                 True             True             True  \n",
      "2                 True             True             True  \n",
      "3                 True             True             True  \n",
      "4                 True             True             True  \n",
      "...                ...              ...              ...  \n",
      "3529              True             True             True  \n",
      "3530              True             True             True  \n",
      "3531              True             True             True  \n",
      "3532              True             True             True  \n",
      "3533              True             True             True  \n",
      "\n",
      "[3534 rows x 9 columns]\n",
      "Training Data has been read\n",
      "Testing Data has been read\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "dataset = pd.read_csv(\"TrainingData.csv\", sep = \",\", encoding=\"latin-1\")\n",
    "testingset = pd.read_csv(\"TestingData.csv\", sep = \",\", encoding=\"latin-1\")\n",
    "testingset = pd.DataFrame(testingset)\n",
    "testingset = testingset.dropna()\n",
    "print(testingset.notnull())\n",
    "print(\"Training Data has been read\")\n",
    "print(\"Testing Data has been read\")\n",
    "dataset.text=dataset.text.astype(str)\n",
    "testingset.text=testingset.text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text :: \n",
      "\n",
      " 0                                            id respond go\n",
      "1                                  sooo sad miss san diego\n",
      "2                                               boss bulli\n",
      "3                                      interview leav alon\n",
      "4                    son couldnt put releas alreadi bought\n",
      "                               ...                        \n",
      "27476    wish could come see u denver husband lost job ...\n",
      "27477    ive wonder rake client made clear net dont for...\n",
      "27478    yay good enjoy break probabl need hectic weeke...\n",
      "27479                                                worth\n",
      "27480                           flirt go atg smile yay hug\n",
      "Name: text, Length: 27481, dtype: object\n",
      "Processed test text :: \n",
      "\n",
      " 0                    last session day httptwitpiccom67ezh\n",
      "1       shanghai also realli excit precis skyscrap gal...\n",
      "2       recess hit veroniqu branquinho quit compani shame\n",
      "3                                              happi bday\n",
      "4                                httptwitpiccom4w75p like\n",
      "                              ...                        \n",
      "3529                             3 im tire cant sleep tri\n",
      "3530    alon old hous thank net keep aliv kick whoever...\n",
      "3531    know mean littl dog sink depress want move som...\n",
      "3532            sutra next youtub video gon na love video\n",
      "3533           httptwitpiccom4woj2 omgssh ang cute ng bbi\n",
      "Name: text, Length: 3534, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing data before feeding it to ML models\n",
    "processed_text = dataset['text'].apply(preprocess_tweet_text)\n",
    "processed_test_text = testingset['text'].apply(preprocess_tweet_text)\n",
    "print(\"Processed text :: \\n\\n\", processed_text)\n",
    "print(\"Processed test text :: \\n\\n\",processed_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = PorterStemmer()\n",
    "# stemmed_words = [stemmer.stem(i) for i in processed_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer = WordNetLemmatizer()\n",
    "# lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "tf_vector = get_feature_vector(np.array(dataset[\"text\"]).ravel())\n",
    "vec_file = 'vectorizer.pickle'\n",
    "pickle.dump(tf_vector, open(vec_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf_vector.transform(np.array(dataset[\"text\"]).ravel())     # Predictor Variable\n",
    "y_train = np.array(dataset[\"sentiment\"]).ravel()                     # Target varaible\n",
    "X_test = tf_vector.transform(np.array(testingset[\"text\"]).ravel()) \n",
    "y_test = np.array(testingset[\"sentiment\"]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPlitting the data into training and testing data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, multi_class='ovr')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Logistics Regression model\n",
    "LR_model = LogisticRegression(max_iter=1000,multi_class=\"ovr\")\n",
    "LR_model.fit(X_train, y_train)\n",
    "# LR_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = [    \n",
    "#     {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "#     'C' : np.logspace(-4, 4, 20),\n",
    "#     'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n",
    "#     'max_iter' : [100, 1000,2500, 5000]\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = GridSearchCV(LR_model, param_grid = param_grid, cv = 3, verbose=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_clf = clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (f'Accuracy - : {best_clf.score(X,y):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Logisitic Regression Model is ::  0.7099603848330504\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Values :\n",
    "\n",
    "y_predict_lr = LR_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy Score for Logisitic Regression Model is :: \",accuracy_score(y_test, y_predict_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for MultinomialNB Model is ::  0.6262026032823995\n"
     ]
    }
   ],
   "source": [
    "MNB_model = MultinomialNB()\n",
    "MNB_model.fit(X_train,y_train)\n",
    "y_predict_mnb = MNB_model.predict(X_test)\n",
    "print(\"Accuracy Score for MultinomialNB Model is :: \",accuracy_score(y_test, y_predict_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for SVC Model is ::  0.7178834182229767\n"
     ]
    }
   ],
   "source": [
    "SVC = SVC()\n",
    "SVC.fit(X_train,y_train)\n",
    "y_predict_svc = SVC.predict(X_test)\n",
    "print(\"Accuracy Score for SVC Model is :: \",accuracy_score(y_test, y_predict_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Linear_SVC Model is ::  0.6861912846632711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "L_SVC = LinearSVC()\n",
    "lol=OneVsRestClassifier(LinearSVC(random_state=0))\n",
    "lol.fit(X_train,y_train)\n",
    "y_predict_lsvc = lol.predict(X_test)\n",
    "print(\"Accuracy Score for Linear_SVC Model is :: \",accuracy_score(y_test, y_predict_lsvc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Bernoulli Model is ::  0.6859083191850595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "B = RandomForestClassifier()\n",
    "B.fit(X_train,y_train)\n",
    "y_predict_b = B.predict(X_test)\n",
    "print(\"Accuracy Score for Bernoulli Model is :: \",accuracy_score(y_test, y_predict_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Bernoulli Model is ::  0.6471420486700622\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "DTC = GradientBoostingClassifier()\n",
    "DTC.fit(X_train,y_train)\n",
    "y_predict_dtc = DTC.predict(X_test)\n",
    "print(\"Accuracy Score for Bernoulli Model is :: \",accuracy_score(y_test, y_predict_dtc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "import pickle\n",
    "filename = 'SVC_Model.pkl'\n",
    "pickle.dump(SVC, open(filename, 'wb')) #wb = write binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading model\n",
    "loaded_model = pickle.load(open(filename, 'rb')) #rb = read binary\n",
    "#loading Vectorizer\n",
    "vectorizer = pickle.load(open('vectorizer.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_input = [\"I have seen many movies and this was not good \"]\n",
    "new_input = vectorizer.transform(new_input)\n",
    "new_output = loaded_model.predict(new_input)\n",
    "new_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
